{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################################################################\n",
    "#Tracing Paths\n",
    "\n",
    "#To use this on your own, make sure you've run the ReactionModeling script first to compile the tracer results, before you can\n",
    "#.. query the network to construct paths\n",
    "\n",
    "#This script will use the dictionaries from the local directory, so if you build e.g. 13C6 glucose tracer results, and want to\n",
    "#... query that network, then have those dictionaries here\n",
    "\n",
    "#This needs to be run in Linux, and multiple cores (e.g. 4 or more) with 16GB+ of RAM is HIGHLY recommended\n",
    "#Note you will need to install pathos and dill packages, and pandas needs to be v0.19 or higher\n",
    "###############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy, pandas, os, sys, re, itertools, csv, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain, repeat, combinations, permutations, cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathos.helpers import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from contextlib import closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load all the dictionaries\n",
    "ResultsList=[]\n",
    "\n",
    "#You will likey not want to test beyond 15 steps anyway given the number of possible paths, but you can change the number of \n",
    "#.. dictionaries to read in here\n",
    "\n",
    "for MyDict in range(15):\n",
    "    ResultsList.append(pickle.load(open('Dictionary_FromRound_{0}.pkl'.format(MyDict+1),'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Read in file for name conversion, make dictionaries\n",
    "CpdConvert=pandas.read_csv('CpdConvertDict.csv')\n",
    "CpdConvert=CpdConvert.dropna()\n",
    "MetaToCommon=dict(zip(CpdConvert.Name,CpdConvert.Common))\n",
    "CommonToMeta=dict(zip(CpdConvert.Common,CpdConvert.Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in file for pubchem smiles to common name\n",
    "PubChemCpds=pandas.read_csv('PubChemCpdMatchAdd.csv',sep=',')\n",
    "\n",
    "\n",
    "PubChemCpdsDict=dict(zip(PubChemCpds.Smiles,PubChemCpds.Name))\n",
    "\n",
    "#Old version\n",
    "\n",
    "#This form of NameToPubChemSmiles builds lists of Common name:SMILES in case \n",
    "#.. there are multiple entries for the same metabolite (e.g. 'DL-Serine')\n",
    "NameToPubChemSmiles=defaultdict(list)\n",
    "for cpd in range(len(PubChemCpds)):\n",
    "    NameToPubChemSmiles[PubChemCpds.Name[cpd]].append(PubChemCpds.Smiles[cpd])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Old function - get the smiles from a pubchem name ID\n",
    "def SmilesFromMetaCycCpd(OneCpdString):\n",
    "    \n",
    "    #Get rid of the bracketing\n",
    "    OneCpdString=SmilesStringRe1.sub(repl='',string=OneCpdString)\n",
    "    \n",
    "    OneCpdString=SmilesStringRe2.sub(repl='',string=OneCpdString)\n",
    "    \n",
    "    #remove extra apostrophes if they exist\n",
    "    if OneCpdString.startswith('\\'') and OneCpdString.endswith('\\''):\n",
    "        OneCpdString=SmilesStringRe3.sub(repl='',string=OneCpdString)\n",
    "        \n",
    "    return(OneCpdString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert user input to Smiles - return list of labeled SMILES options\n",
    "def SmilesOptionsFromUserInput(userinput):\n",
    "    try:\n",
    "        #Convert the user input to the MetaCyc-compatible name\n",
    "        Conversion1=CommonToMeta[re.sub(' M\\+\\d','',userinput)]\n",
    "        Conversion2=NameToPubChemSmiles[Conversion1]\n",
    "        return(Conversion2)\n",
    "    except:\n",
    "        print('Oops, no match of the user input to Pubchem Smiles, check your spelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Possibilities level off for metabolites, e.g. Serine starts duplicating after round 12, so only need to calculate paths until then\n",
    "def FinalRoundMetabLength(ResultsDictionaries,MetabName):\n",
    "    ResultDFs=[]\n",
    "    \n",
    "    #Build the dataframes for each round\n",
    "    for resultdict in range(len(ResultsDictionaries)):\n",
    "        ResultDFs.append(BuildOneRoundPath(ResultsList[resultdict],MetabName))\n",
    "    \n",
    "    #Check if consecutive rounds are equal\n",
    "    for rxnround in reversed(range(len(ResultDFs))):\n",
    "        if all(ResultDFs[rxnround][2].isin(ResultDFs[rxnround-1][2]))==False and len(ResultDFs[rxnround-1]>0):\n",
    "            Length=rxnround\n",
    "            break\n",
    "    return(Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combinations of isotopomers may exist for the same compound \n",
    "#    e.g. metabolite X M+2 could be C*(C(C*)OOO), or C*(C*(C)OOO), so we need to run a combinations function to get all possible isotopologues\n",
    "#If you want to run a single isotopomer (say you know positional 13C labeling), then you can skip this part and run the BuildPathsCoreParallel\n",
    "#... function manually\n",
    "\n",
    "\n",
    "#Need number of carbons, number of user input labeled carbons\n",
    "def IsotopomerCombinations(CpdSmiles,NumOfLabels):\n",
    "    \n",
    "    MasterIsotopomerList=[]\n",
    "    \n",
    "    for cpd in CpdSmiles:\n",
    "    \n",
    "        #Find all the carbons in the original Smiles input\n",
    "        CarbonList=re.findall('C',cpd)\n",
    "    \n",
    "        #Set up the first permutation of labels by adding the specified number of 13Cs\n",
    "        LabelCarbonList=[re.sub('C','C*',x) for x in CarbonList[0:NumOfLabels]]\n",
    "    \n",
    "        #Replace the carbons with labeled carbons\n",
    "        CarbonList[0:NumOfLabels]=LabelCarbonList\n",
    "    \n",
    "        #Make permutations of isotopomers\n",
    "        CarbonPerms=[x for x in permutations(CarbonList)]\n",
    "    \n",
    "        #Build into list\n",
    "        CarbonPerms=[list(x) for x in list(set(CarbonPerms))]\n",
    "    \n",
    "        #Find the indicies of carbons in original Smiles, this will get replaced by 13C\n",
    "        ReplList=[m.start() for m in re.finditer('C',cpd)]\n",
    "    \n",
    "        #Listify the string for positional replacement\n",
    "        ListifySmiles=[x for x in cpd]\n",
    "    \n",
    "        #List to be built of all permutations of isotopomers\n",
    "        NewListOfLabeledSmiles=[]\n",
    "    \n",
    "        #For each isotopomer permutation\n",
    "        for Perms in range(len(CarbonPerms)):\n",
    "        \n",
    "            #Ordered list of the given isotopomer\n",
    "            LabList=CarbonPerms[Perms]\n",
    "        \n",
    "            #For each index to be replaced\n",
    "            for position in range(len(ReplList)):\n",
    "            \n",
    "                #Replace with C* if necessary\n",
    "                ListifySmiles[ReplList[position]]=LabList[position]\n",
    "            \n",
    "            #Rebuild into string\n",
    "            cpd=''.join(ListifySmiles)\n",
    "        \n",
    "            #Add string to all possible isotopomers\n",
    "            NewListOfLabeledSmiles.append(cpd)\n",
    "        \n",
    "        MasterIsotopomerList.extend(NewListOfLabeledSmiles)\n",
    "        \n",
    "    return(MasterIsotopomerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate longest path for each isotopomer - cap at 15 due to number of possibilities\n",
    "def CalculateIsotopomerLength(ResultsDictionaries,IsotopomerComboList):\n",
    "    Lengths=[]\n",
    "    for isotope in IsotopomerComboList:\n",
    "        try:\n",
    "            Lengths.append(FinalRoundMetabLength(ResultsDictionaries,isotope))\n",
    "        except:\n",
    "            pass\n",
    "    if max(Lengths)>15: \n",
    "        return(15)\n",
    "    else:\n",
    "        return(max(Lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to remove charges and unwanted characters in smiles strings\n",
    "ChargeRemover=re.compile(r'\\+|\\-')\n",
    "ExtraPostasRemover=re.compile('^\\'|\\'$')\n",
    "\n",
    "def LabelStrippersOne(x):\n",
    "    \n",
    "    #Remove labels\n",
    "    x=str(x).replace('*','')\n",
    "    \n",
    "    #Remove extra Rs?\n",
    "    x=x.replace('R','')\n",
    "    \n",
    "    #Remove charges\n",
    "    x=ChargeRemover.sub('',string=str(x))\n",
    "    \n",
    "    #Remove any rextra apostrophes\n",
    "    x=ExtraPostasRemover.sub('',string=str(x))\n",
    "    \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Name converter from Smiles to 'English'\n",
    "def ReactantConvertOne(x):\n",
    "    try:\n",
    "        x=PubChemCpdsDict[x]\n",
    "    except:\n",
    "        x='nomatch'\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CombinedConvertMetabs(OneLabeledMatrixCpd):\n",
    "    \n",
    "    #Get number of label counts\n",
    "    OneLabeledMatrixCpdCarbs=str(OneLabeledMatrixCpd).count('*')\n",
    "    \n",
    "    #Strip labels and charges\n",
    "    OneLabeledMatrixCpdsEnglish=LabelStrippersOne(OneLabeledMatrixCpd)\n",
    "    \n",
    "    #Convert to English using metabolite dictionary\n",
    "    OneLabeledMatrixCpdsEnglish=ReactantConvertOne(OneLabeledMatrixCpdsEnglish)\n",
    "    \n",
    "    #Format string to use both english name and label number\n",
    "    OneLabeledMatrixCpd=str('{0} M+{1}').format(OneLabeledMatrixCpdsEnglish,OneLabeledMatrixCpdCarbs)\n",
    "    \n",
    "    return(OneLabeledMatrixCpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to call the conversion of reactants and products to english\n",
    "def UpdatedMetabConvert(LabeledMatrixCpds):\n",
    "    LabeledMatrixCpds=[CombinedConvertMetabs(x) for x in LabeledMatrixCpds]\n",
    "    return(LabeledMatrixCpds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Trim reaction matrix to drop nonmatching metabolites (i.e. they can't be named)\n",
    "def TrimExportReactionMatrix(ExportReactionMatrix):\n",
    "\n",
    "    #Drop the nans\n",
    "    ExportReactionMatrix=ExportReactionMatrix.dropna(subset=['Reactants'])\n",
    "    ExportReactionMatrix=ExportReactionMatrix.dropna(subset=['Products'])\n",
    "    \n",
    "    #drop row indicies that have a blank Labeled Reactants or Products cell\n",
    "    ExportReactionMatrix=ExportReactionMatrix.drop(ExportReactionMatrix[ExportReactionMatrix['Reactants'].map(len)==0].index,axis=0)\n",
    "    ExportReactionMatrix=ExportReactionMatrix.drop(ExportReactionMatrix[ExportReactionMatrix['Products'].map(len)==0].index,axis=0)\n",
    "\n",
    "    return(ExportReactionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If metabolite didn't get an english name match, drop it\n",
    "def DropMissingMetabs(ConvertedMatrixColumnCpd):\n",
    "\n",
    "    ConvertedMatrixColumnCpd=[item for item in ConvertedMatrixColumnCpd if not re.search('^ M+[0-9]*',item)]\n",
    "\n",
    "    return(ConvertedMatrixColumnCpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taken from  itertools guide\n",
    "def islice(iterable, *args):\n",
    "    s = slice(*args)\n",
    "    it = iter(range(s.start or 0, s.stop or sys.maxsize, s.step or 1))\n",
    "    try:\n",
    "        nexti = next(it)\n",
    "    except StopIteration:\n",
    "        return\n",
    "    for i, element in enumerate(iterable):\n",
    "        if i == nexti:\n",
    "            yield element\n",
    "            nexti = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taken from itertools guide\n",
    "def roundrobin(*iterables):\n",
    "    #\"roundrobin('ABC', 'D', 'EF') --> A D E B F C\"\n",
    "    # Recipe credited to George Sakkis\n",
    "    pending = len(iterables)\n",
    "    nexts = cycle(iter(it).__next__ for it in iterables)\n",
    "    while pending:\n",
    "        try:\n",
    "            for next in nexts:\n",
    "                yield next()\n",
    "        except StopIteration:\n",
    "            pending -= 1\n",
    "            nexts = cycle(islice(nexts, pending))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert each isotopomer in a row to the common name with the M+x in tact\n",
    "def IsotopomerToCommonName(OneRowPathMatrix):\n",
    "    ConvertList=[CombinedConvertMetabs(x) for x in OneRowPathMatrix.values[0]if not type(x)==list] #good\n",
    "    NonConvertList=[x for x in OneRowPathMatrix.values[0] if type(x)==list]\n",
    "    OneRowPathMatrix=pandas.DataFrame(list(roundrobin(ConvertList,NonConvertList))).T\n",
    "    return(OneRowPathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove metabolites which cannot be name-converted from smiles\n",
    "def RemoveTheUnnamed(OnePathMatrix):\n",
    "    DropIndices=[]\n",
    "    for rxn in range(len(OnePathMatrix)):\n",
    "        if any([x for x in OnePathMatrix[rxn:rxn+1].values[0] if 'nomatch' in x]):\n",
    "            DropIndices.append(rxn)\n",
    "    OnePathMatrix=OnePathMatrix.drop(DropIndices)\n",
    "    OnePathMatrix=OnePathMatrix.reset_index(drop=True)\n",
    "    return(OnePathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert path matrix to common name\n",
    "def ConvertPathMatrixToEnglish(OnePathMatrix):\n",
    "    NewMatrix=pandas.DataFrame()\n",
    "    for row in range(len(OnePathMatrix)):\n",
    "        NewMatrix=NewMatrix.append(IsotopomerToCommonName(OnePathMatrix[row:row+1]))\n",
    "    NewMatrix=NewMatrix.reset_index(drop=True)\n",
    "    NewMatrix=RemoveTheUnnamed(NewMatrix)\n",
    "    return(NewMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#With a given input isotopologue name, calculate the possible isotopomer combinations and run each through the BuildPathsCoreParallel function\n",
    "def BuildPathsSetupIsotopomers(Dictionaries,userinputname,PathLength):\n",
    "    \n",
    "    #Get the smiles of interest\n",
    "    CpdMatch=SmilesOptionsFromUserInput(userinputname)\n",
    "    \n",
    "    #Get M+x\n",
    "    IsotopologueNum=re.findall('M\\+\\d',userinputname)[0]\n",
    "    \n",
    "    #Get number of labeled carbons\n",
    "    NumOfLabels=int(re.findall('\\d',IsotopologueNum)[0])\n",
    "    \n",
    "    #Generate cominbations of isotopomers to search for\n",
    "    IsotopomerList=IsotopomerCombinations(CpdMatch,NumOfLabels)\n",
    "    \n",
    "    if PathLength=='':\n",
    "    \n",
    "        #Get the max path length - note this is currently capped at 15\n",
    "        StopDict=CalculateIsotopomerLength(Dictionaries,IsotopomerList) #subtract one to correctly index ReusltList - oct 2017\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        StopDict=PathLength #same as above for the 1 subtraction - oct 2017\n",
    "    \n",
    "    #Hard set the length\n",
    "    #StopDict=11\n",
    "\n",
    "    for Isotopomer in IsotopomerList:\n",
    "        try:\n",
    "            for SubLength in list(reversed(range(StopDict))):\n",
    "                print('Calculating',Isotopomer,'Paths of Length',SubLength+1) #adding one to index the correct round - oct 2017\n",
    "                BuildPathsCoreParallel(SubLength,Isotopomer,userinputname,StopDict)\n",
    "        \n",
    "        except:\n",
    "            print('Isotopomer failed')\n",
    "            continue\n",
    "                    \n",
    "    #Given the parallel form of this script, the DF should be read back in\n",
    "    #.. and duplicate rows should be dropped\n",
    "    try:\n",
    "        TempDF=pandas.read_csv('{0}_Paths_{1}Rxns.csv'.format(userinputname,PathLength),header=None,error_bad_lines=False)\n",
    "        TempDF=TempDF.drop_duplicates()\n",
    "        TempDF=TempDF.reset_index(drop=True)\n",
    "        TempDF=DropDuplicatedMetabNamesPostCalc(TempDF)\n",
    "        with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,PathLength),'w') as fp: #'w' to overwrite the file\n",
    "            TempDF.to_csv(fp,index=False,header=False)\n",
    "        fp.close()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop any duplicate name within the same row - this function is to remove futile cycling in metabolic path searches\n",
    "def DropDuplicatedMetabNamesPostCalc(SeedPathMatrix):\n",
    "    DropIndices=[]\n",
    "    for rxn in range(len(SeedPathMatrix)):\n",
    "        if len(list(set([x for x in SeedPathMatrix[rxn:rxn+1].values[0] if 'M+' in x])))!=len(list([x for x in SeedPathMatrix[rxn:rxn+1].values[0] if 'M+' in x])): \n",
    "            DropIndices.append(rxn)\n",
    "    SeedPathMatrix=SeedPathMatrix.drop(DropIndices)\n",
    "    SeedPathMatrix=SeedPathMatrix.reset_index(drop=True)\n",
    "    return(SeedPathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a DF that contains the product of interest and the res\n",
    "def BuildOneRoundPath(ResultDictionary,MetabName):\n",
    "    \n",
    "    #Get around key error problem\n",
    "    try:\n",
    "        #Get the reactants and enzymes key'ed to the metabolite of interest\n",
    "        ReactantList=list(ResultDictionary[MetabName].keys())\n",
    "        EnzymeList=list(ResultDictionary[MetabName].values())\n",
    "    except:\n",
    "        ReactantList=['']\n",
    "        EnzymeList=['']\n",
    "    \n",
    "    #Repeated list of original MetabName to match into a pandas DF\n",
    "    MetabNameFlat=list(repeat(MetabName,len(ReactantList)))\n",
    "    \n",
    "    #Build DF\n",
    "    Output=pandas.DataFrame([MetabNameFlat,EnzymeList,ReactantList]).T\n",
    "    return(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ProductDictionaryResult is the result of one round of BuildOneRoundPath\n",
    "#Prior round will be the x-1 round of the x'th round which built the ProductDictionaryResult\n",
    "def AddOnPath(ProductDictionaryResult,PriorRoundDictionary):\n",
    "\n",
    "    #Make sure the paths exist first, otherwise just exit the function\n",
    "    if ProductDictionaryResult is None:\n",
    "        return\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        #Build a list of new matrices - using last column\n",
    "        TempDF=[BuildOneRoundPath(PriorRoundDictionary,x) for x in ProductDictionaryResult[ProductDictionaryResult.columns[-1]]]\n",
    "    \n",
    "        #Faster here than below?\n",
    "        #Remove the paths which cannot be connected (which have blank fields)\n",
    "        TempDF=[x for x in TempDF if not '' in x.values]\n",
    "    \n",
    "        #Consider a situaion where the product has no match in the ResultsList Dictionary\n",
    "        #... we'll decide to either return nothing or continue filling the DF\n",
    "    \n",
    "        if len(TempDF)==0:\n",
    "            Output=None\n",
    "    \n",
    "        else:\n",
    "    \n",
    "            #Extend the prior rounds' rows to match the new TempDF so column-binding can be performed\n",
    "            ExtendedProductDictionaryResult=[list(repeat(ProductDictionaryResult.iloc[x][0:len(ProductDictionaryResult.columns)-1],len(TempDF[x]))) for x in range(len(TempDF))]\n",
    "            ExtendedProductDictionaryResult=list(chain.from_iterable(ExtendedProductDictionaryResult))\n",
    "        \n",
    "            if len(ExtendedProductDictionaryResult)==1:\n",
    "                ExtendedProductConcat=pandas.DataFrame(pandas.concat([x for x in ExtendedProductDictionaryResult])).T\n",
    "            else:\n",
    "                ExtendedProductConcat=pandas.concat([x for x in ExtendedProductDictionaryResult],axis=1).T\n",
    " \n",
    "            if len(TempDF)>1:\n",
    "                TempDFConcat=pandas.concat([x for x in TempDF])\n",
    "            else:\n",
    "                TempDFConcat=TempDF[0]\n",
    "            \n",
    "            ExtendedProductConcat=ExtendedProductConcat.reset_index(drop=True)\n",
    "            TempDFConcat=TempDFConcat.reset_index(drop=True)\n",
    "\n",
    "            Output=pandas.concat([ExtendedProductConcat,TempDFConcat],axis=1)\n",
    "    \n",
    "            #Drop rows with blank fields\n",
    "            #Output=Output[Output[Output.columns[-1]].values!='']\n",
    "    \n",
    "            #Sorbose problem.. rename it\n",
    "            Output=Output.replace(to_replace='Sorbose',value='Sorbose, L- M+6')\n",
    "    \n",
    "            return(Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write a function to convert any metabolite names in an output file to more common names\n",
    "#Its ugly but it works, lets assume dataframes are not millions of rows\n",
    "def ConvertNames(PathMatrix):\n",
    "    NewMatrix=pandas.DataFrame()\n",
    "    for rxns in range(len(PathMatrix)):\n",
    "        OneRow=PathMatrix[rxns:rxns+1]\n",
    "        OneRow=OneRow.reset_index(drop=True)\n",
    "        for x in OneRow:\n",
    "            try: \n",
    "                carbs=re.findall('M\\+\\d',OneRow.loc[0][x])[0]\n",
    "                convertname=MetaToCommon[re.sub(' M\\+\\d','',OneRow.loc[0][x])]\n",
    "                metabname=convertname+' '+carbs\n",
    "                OneRow.loc[0][x]=metabname\n",
    "            except:\n",
    "                pass\n",
    "        NewMatrix=NewMatrix.append(OneRow,ignore_index=True)\n",
    "    return(NewMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop any duplicate name within the same row - this function is to remove futile cycling in metabolic path searches\n",
    "def DropDuplicatedMetabNamesAny(SeedPathMatrix):\n",
    "    DropIndices=[]\n",
    "    for rxn in range(len(SeedPathMatrix)):\n",
    "        if len(list(set([x for x in SeedPathMatrix[rxn:rxn+1].values[0] if '*' in x])))!=len(list([x for x in SeedPathMatrix[rxn:rxn+1].values[0] if '*' in x])): \n",
    "            DropIndices.append(rxn)\n",
    "    SeedPathMatrix=SeedPathMatrix.drop(DropIndices)\n",
    "    SeedPathMatrix=SeedPathMatrix.reset_index(drop=True)\n",
    "    return(SeedPathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start with a function to build the paths from a chosen ending point, then this will be embedded in a larger function that\n",
    "#starts from each possible ending point (e.g. Round 1-20)\n",
    "\n",
    "def BuildPathsCoreNonParallel(StopDictNum,SeedPathMatrix,MetabName):\n",
    "        \n",
    "    \n",
    "    #Build paths going backwards, back to the input tracer\n",
    "    for rxnround in list(reversed(range(StopDictNum))):\n",
    "        \n",
    "        #Add on one round to the existing routes\n",
    "        SeedPathMatrix=AddOnPath(SeedPathMatrix,ResultsList[rxnround])\n",
    "        \n",
    "    #Reset column names to a numbered order to append shorter metabolite routes later\n",
    "    SeedPathMatrix.columns=range(len(SeedPathMatrix.columns))  \n",
    "    \n",
    "    SeedPathMatrix=SeedPathMatrix.reset_index(drop=True)\n",
    "    \n",
    "    #Drop any duplicate metab name - added Jan29\n",
    "    SeedPathMatrix=DropDuplicatedMetabNamesAny(SeedPathMatrix)\n",
    "\n",
    "    #Start from one reaction earlier - loop from 1 to StopDict to build progressively shorter paths\n",
    "    for subrxn in range(1,StopDictNum,1):\n",
    "        StartingSubSet=BuildOneRoundPath(ResultsList[StopDictNum-subrxn],MetabName)\n",
    "        if len(StartingSubSet)>0:\n",
    "            SeedPathSub=AddOnPath(StartingSubSet,ResultsList[StopDictNum-subrxn-1])\n",
    "            for rxnround in list(reversed(range(StopDictNum-subrxn-1))):\n",
    "                SeedPathSub=AddOnPath(SeedPathSub,ResultsList[rxnround])\n",
    "                \n",
    "                #If an extension cannot be made in the short path search\n",
    "                if len(SeedPathSub)==0:\n",
    "                    del(SeedPathSub)\n",
    "                \n",
    "                else:\n",
    "                    SeedPathSub=SeedPathSub.reset_index(drop=True)\n",
    "                    #Drop any duplicate metab name - added Jan29\n",
    "                    SeedPathSub=DropDuplicatedMetabNamesAny(SeedPathSub)\n",
    "                \n",
    "        if 'SeedPathSub' in locals():\n",
    "            SeedPathSub.columns=range(len(SeedPathSub.columns))\n",
    "            SeedPathMatrix=SeedPathMatrix.append(SeedPathSub,ignore_index=True)\n",
    "\n",
    "    #Convert metab names from SMILES\n",
    "    SeedPathMatrix=ConvertPathMatrixToEnglish(SeedPathMatrix)\n",
    "\n",
    "    #Convert ugly metabolite names        \n",
    "    SeedPathMatrix=ConvertNames(SeedPathMatrix)\n",
    "    \n",
    "    #Write to file\n",
    "    #SeedPath.to_csv('{0}_Paths.csv'.format(userinputname))\n",
    "    return(SeedPathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AddOnPathParallel3(SplitMatrix,StopDictNum):\n",
    "    #Build paths going backwards, back to input tracer\n",
    "    for rxnround in list(reversed(range(StopDictNum-1))):\n",
    "        \n",
    "        #Add on one round to the existing routes\n",
    "        SplitMatrix=AddOnPath(SplitMatrix,ResultsList[rxnround])\n",
    "        \n",
    "        #Function to remove any rows which contain multiple instances of MetabName, avoid metabolic cycles/loops\n",
    "        SplitMatrix=DropDuplicatedMetabNamesAny(SplitMatrix) #Replaced Jan29\n",
    "\n",
    "    #Reset column names to a numbered order to append shorter metabolite routes later\n",
    "    SplitMatrix.columns=range(len(SplitMatrix.columns))\n",
    "    \n",
    "    SplitMatrix=SplitMatrix.reset_index(drop=True)\n",
    "    \n",
    "    SplitMatrix=DropDuplicatedMetabNamesAny(SplitMatrix)\n",
    "    \n",
    "    if len(SplitMatrix)==0:\n",
    "            \n",
    "        return(SplitMatrix)\n",
    "            \n",
    "    else:\n",
    "        SplitMatrix.columns=range(len(SplitMatrix.columns))\n",
    "        \n",
    "        #Convert from SMILES\n",
    "        SplitMatrix=ConvertPathMatrixToEnglish(SplitMatrix)\n",
    "        #Convert ugly names            \n",
    "        SplitMatrix=ConvertNames(SplitMatrix)\n",
    "                \n",
    "        return(SplitMatrix)\n",
    "    \n",
    "    #return(SplitMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Current\n",
    "#Input with the particular dictionary round of interest\n",
    "def BuildPathsCoreParallel(ResultDictionaryNum,metabname,userinputname,StopDictNum):\n",
    "    \n",
    "    \n",
    "    #The starting set of reactants that led to the product isotopomer of interest\n",
    "    StartingSet=BuildOneRoundPath(ResultsList[ResultDictionaryNum],metabname)\n",
    "    \n",
    "    if len(StartingSet)>0:\n",
    "    \n",
    "        #Starter path matrix\n",
    "        SeedPath=AddOnPath(StartingSet,ResultsList[ResultDictionaryNum-1])\n",
    "        \n",
    "        if SeedPath is not None:\n",
    "            try:\n",
    "                #Go one more path matrix\n",
    "                SeedPath=AddOnPath(SeedPath,ResultsList[ResultDictionaryNum-2])\n",
    "    \n",
    "                #if len(SeedPath)>48:\n",
    "    \n",
    "                SeedPathSplit=list(numpy.array_split(SeedPath,32))\n",
    "\n",
    "                StopDictRepeat=list(repeat(ResultDictionaryNum-2,32))\n",
    "            \n",
    "                pooler=mp.Pool(16)\n",
    "                        \n",
    "                try:\n",
    "                        \n",
    "                    with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,StopDictNum),'a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "        \n",
    "                        for result in pooler.starmap(AddOnPathParallel3,zip(SeedPathSplit,StopDictRepeat)):\n",
    "                            result.to_csv(fp,index=False,header=False)\n",
    "                    pooler.close()\n",
    "                    pooler.join()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except:\n",
    "                        \n",
    "                    try:\n",
    "                                \n",
    "                        SeedPathSplit=list(numpy.array_split(SeedPath,16))\n",
    "                        StopDictRepeat=list(repeat(ResultDictionaryNum-2,16))\n",
    "                            #pooler=mp.Pool(8)\n",
    "                            #with closing(mp.Pool(8)) as pooler:\n",
    "                        with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,StopDictNum),'a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "                            for result in pooler.starmap(AddOnPathParallel3,zip(SeedPathSplit,StopDictRepeat)):\n",
    "                                result.to_csv(fp,index=False,header=False)\n",
    "\n",
    "                        pooler.close()\n",
    "                        pooler.join()\n",
    "                        gc.collect()\n",
    "                            \n",
    "                    except:\n",
    "\n",
    "                        #print('Error 1- Some of these paths cannot be connected, try another isotopologue if no results are written to csv [meaning no paths could be connected]')\n",
    "                        \n",
    "                        try:\n",
    "                                #Try smaller splits\n",
    "                            SeedPathSplit=list(numpy.array_split(SeedPath,8))\n",
    "                            StopDictRepeat=list(repeat(ResultDictionaryNum-2,8))\n",
    "                                #pooler=mp.Pool(4)\n",
    "                            with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,StopDictNum),'a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "                                for result in pooler.starmap(AddOnPathParallel3,zip(SeedPathSplit,StopDictRepeat)):\n",
    "                                    result.to_csv(fp,index=False,header=False)\n",
    "                    \n",
    "                            pooler.close()\n",
    "                            pooler.join()\n",
    "                            gc.collect()\n",
    "                        \n",
    "                        except:\n",
    "\n",
    "                            #print('Error 2- Some of these paths cannot be connected, try another isotopologue if no results are written to csv [meaning no paths could be connected]')\n",
    "                        \n",
    "                            try:\n",
    "                                #Try smaller splits\n",
    "                                SeedPathSplit=list(numpy.array_split(SeedPath,4))\n",
    "                                StopDictRepeat=list(repeat(ResultDictionaryNum-2,4))\n",
    "                                #pooler=mp.Pool(4)\n",
    "                                with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,StopDictNum),'a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "                                    for result in pooler.starmap(AddOnPathParallel3,zip(SeedPathSplit,StopDictRepeat)):\n",
    "                                        result.to_csv(fp,index=False,header=False)\n",
    "                    \n",
    "                                pooler.close()\n",
    "                                pooler.join()\n",
    "                                gc.collect()\n",
    "            \n",
    "                            except:\n",
    "                \n",
    "                                try:\n",
    "                                #Try smaller split\n",
    "                                    SeedPathSplit=list(numpy.array_split(SeedPath,2))\n",
    "                                    StopDictRepeat=list(repeat(ResultDictionaryNum-2,2))\n",
    "                            #pooler=mp.Pool(2)\n",
    "                                    with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,StopDictNum),'a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "                                        for result in pooler.starmap(AddOnPathParallel3,zip(SeedPathSplit,StopDictRepeat)):\n",
    "                                            result.to_csv(fp,index=False,header=False)\n",
    "                                    pooler.close()\n",
    "                                    pooler.join()\n",
    "                                    gc.collect()\n",
    "                    \n",
    "                                except:\n",
    " \n",
    "                                    #print('Error 3 - Some of these paths cannot be connected, try another isotopologue if no results are written to csv [meaning no paths could be connected]')\n",
    "                  \n",
    "                                    try:\n",
    "                                        #print('No Parallel')\n",
    "                                        Output=BuildPathsCoreNonParallel(ResultDictionaryNum-2,SeedPath,metabname)\n",
    "        \n",
    "                                        with open('{0}_Paths_{1}Rxns.csv'.format(userinputname,StopDictNum),'a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "                                            Output.to_csv(fp,index=False,header=False)\n",
    "                                        \n",
    "                                        pooler.close()\n",
    "                                        pooler.join()\n",
    "                                        gc.collect()\n",
    "                    \n",
    "                                    except:\n",
    "                        \n",
    "                                        print('Paths cannot be built')\n",
    "                                        pooler.close()\n",
    "                                        pooler.join()\n",
    "                                        gc.collect()\n",
    "                                    \n",
    "                                        return\n",
    "                        \n",
    "            except:\n",
    "                print('Isotopomer Failed')\n",
    "                pooler.close()\n",
    "                pooler.join()\n",
    "                gc.collect()\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def BuildPaths(): #User input form\n",
    "def BuildPaths(userinput,pathlength):\n",
    "    #userinput=input('Enter isotopologue, or type \"help\" for lists of common isotopologues (then call BuildPaths again with specified isotopologue of interest): ') #User input form\n",
    "    \n",
    "    #pathlength=input('Enter path length to search for (please enter a number)- if nothing is entered, length will be automatically calculated and a max length will be set to 15: ') #User input form\n",
    "    \n",
    "    if pathlength!='':\n",
    "    \n",
    "        pathlength=int(pathlength)\n",
    "    \n",
    "        if pathlength>15:\n",
    "            pathlength=15\n",
    "    \n",
    "    if userinput=='help':\n",
    "        print(CpdConvert.Common)\n",
    "    if userinput!='help' and userinput.startswith('Acetyl-CoA'):\n",
    "        print('The number of possible Acetyl-CoA isotopomers is too great to calculate (e.g. 23 choose 2 for M+2 with a 23-carbon compound), and we recommend manually going into this script and specifying your particular isotpomer of interest')\n",
    "    if userinput!='help' and userinput.startswith('Acetyl-CoA')==False:\n",
    "        try:\n",
    "            \n",
    "            print('Calculating results... this may take a few minutes')\n",
    "            \n",
    "            BuildPathsSetupIsotopomers(ResultsList,userinput,pathlength)\n",
    "                         \n",
    "            print('Calculations finished, if paths could be found, they will be stored as a csv in the local directory')\n",
    "            #quit()\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating results... this may take a few minutes\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n",
      "/Users/srhoades/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 13\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 12\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 11\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 10\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 9\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 8\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 7\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 6\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 5\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 4\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 3\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 2\n",
      "Calculating C*(O)C*(N)C*(=O)O Paths of Length 1\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 14\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 13\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 12\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 11\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 10\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 9\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 8\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 7\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 6\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 5\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 4\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 3\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 2\n",
      "Calculating C*(O)C*(N)C*(O)=O Paths of Length 1\n",
      "Calculations finished, if paths could be found, they will be stored as a csv in the local directory\n"
     ]
    }
   ],
   "source": [
    "#If you want to run a query yourself, an example would be to call BuildPaths('Serine M+3',12)\n",
    "#BuildPaths('Serine M+3',14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
