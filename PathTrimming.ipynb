{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#Using calculated isotopomer paths, two main approaches of trimming will be used\n",
    "#Inputs - Xu2011Gill2015_ECs_Cyclers.csv, Tepper2013_GibbsECs.csv, Serine M+3_Paths_9Rxns.csv, Glutamine M+2_Paths_14Rxns.csv\n",
    "#Outputs - SerM3_Paths_9RxnsFilter.csv, SerM3_Paths_9RxnsNoFilter.csv, GlnM2_Paths_14RxnsNoFilter.csv, GlnM2_Paths_14RxnsNoFilter.csv\n",
    "\n",
    "#1) Using circadian EC hits from Xu, 2011 and Gill, 2015, to pull out any rows\n",
    "#...which contain any ECs, and the number of ECs in a row given will also be added\n",
    "#\n",
    "#2) Gibbs energies have been estimated in Tepper, 2013, which will be used to \n",
    "#...predict path thermodynamics\n",
    "#\n",
    "#Note this is an optional script, and an example of how one may go about trimming\n",
    "#.. excess results through other means, such as previously published 'omic data\n",
    "#.. or reaction thermodynamics\n",
    "#\n",
    "#This script should be used as a guide, as the trimming procedures can be very\n",
    "#.. dependent on context and experimental objectives\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, re, itertools, csv\n",
    "from itertools import chain, repeat\n",
    "from collections import defaultdict\n",
    "import dill as pickle\n",
    "from pathos.helpers import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in a list of significant circadian ECs from Xu et al, 2011 and Gill et al, 2015\n",
    "SigECList=pd.read_csv('Xu2011Gill2015_ECs_Cyclers.csv',header=None)\n",
    "ECList=list(SigECList[0])\n",
    "\n",
    "#Edit names to match the format of the PathSearch results\n",
    "ECList=['EC-'+x for x in ECList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in the list of estimated Gibbs energies for all annotated enzymes, from Tepper et al., 2013\n",
    "GibbsList=pd.read_csv('Tepper2013_GibbsECs.csv',skiprows=1)\n",
    "GibbsECList=['EC-'+x for x in list(GibbsList['ec number'])]\n",
    "#Note this will take the last delta G value in a list of duplicated ECs\n",
    "\n",
    "GibbsListDict=dict(zip(GibbsECList,GibbsList['standart Gibbs energy (kJ/mol)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From the fly studies, drop any paths which don't have circadian enzymes\n",
    "#NOTE!# This step is optional, it is entirely possible that pathway activities may be cyclic without a circadian enzyme (for more detail, see Thurley et al, PNAS, 2017)\n",
    "\n",
    "def KeggECTrimmer(RowItem,RxnNumber):\n",
    "    #Convert the stringy EC item into a list of one/multiple ECs\n",
    "    ListyECs=[re.sub('\\[|\\'|\\]','',x) for x in re.split('\\, ',RowItem)]\n",
    "    \n",
    "    #if any item across the row is found in the circadian enzyme list\n",
    "    if any([y for y in ListyECs if y in KeggECList]):\n",
    "        RowItem=[y for y in ListyECs if y in KeggECList]\n",
    "        return(RowItem)\n",
    "    else:\n",
    "        return(RxnNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find ECs for reporting - if there isn't an EC (say only a Metacyc RXN is left), it can be dropped - of course the 'RXN' format in MetaCyc may represent real reactions, but if it isn't connection to an EC the interpretation is more difficult\n",
    "def MatchFlyKeggECs(PathMatrix):\n",
    "    DropIndices=[]\n",
    "    for rxn in range(len(PathMatrix)):\n",
    "        ECMatchList=[KeggECTrimmer(z,rxn) for z in PathMatrix[rxn:rxn+1].values[0] if type(z) is str and 'EC' in z]\n",
    "        if any([x==rxn for x in ECMatchList]):\n",
    "            DropIndices.append(rxn)\n",
    "            \n",
    "    #Store indicies where there isn't a RXN -> EC conversion\n",
    "    PathMatrix=PathMatrix.drop(DropIndices)\n",
    "    PathMatrix=PathMatrix.reset_index(drop=True)\n",
    "    return(PathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sum up the gibbs energies for a given metabolic path\n",
    "def SumGibbsAcrossRow(PathMatrix):\n",
    "    MiniGibbList=[]\n",
    "    for items in PathMatrix:\n",
    "\n",
    "        try:\n",
    "            if any([GibbsListDict[x] for x in GibbsECList if x in re.split('\\'|\\,',items)]):\n",
    "                \n",
    "                MiniGibbList.append(min([GibbsListDict[x] for x in GibbsECList if x in re.split('\\'|\\,',items)]))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    PathMatrix['Gibbs Sum']=sum(MiniGibbList)\n",
    "    return(PathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Match ECs in isotopomer paths to significant EC hits from Xu,2011 and Gill,2015\n",
    "def CircadianECCounter(PathMatrix):\n",
    "    \n",
    "    TempECList=[]\n",
    "    \n",
    "    for items in PathMatrix:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            if any([x for x in ECList if x in re.split('\\'|\\,',items)])==True:\n",
    "                    \n",
    "                    #Add the EC to the list\n",
    "                TempECList.extend([x for x in ECList if x in re.split('\\'|\\,',items)])\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #Only unique items in the ECList\n",
    "    TempECList=list(set(TempECList))\n",
    "    \n",
    "    PathMatrix['Number of Circadian ECs']=len(TempECList)\n",
    "    PathMatrix['Circadian ECs']=TempECList\n",
    "    \n",
    "    return(PathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to find ECs in the PathMatrix which match to Xu,2011 and Gill,2015 circadian hits\n",
    "#This function is currently very slow for large data frames\n",
    "#Converted to apply functions to take out loops and still slow, some optimization may be required\n",
    "def MatchCircECHitsAndGetGibbs(PathMatrix, Call='Filter'):\n",
    "    \n",
    "    PathMatrix=PathMatrix.reset_index(drop=True)\n",
    "    \n",
    "    #Drop duplicates\n",
    "    PathMatrix=PathMatrix.drop_duplicates()\n",
    "    \n",
    "    PathMatrix=PathMatrix.reset_index(drop=True)\n",
    "    \n",
    "    #ECs added\n",
    "    PathMatrixECs=PathMatrix.apply(CircadianECCounter,axis=1)\n",
    "    \n",
    "    #Caculate Gibbs across each path\n",
    "    PathMatrix=PathMatrix.apply(SumGibbsAcrossRow,axis=1)\n",
    "\n",
    "    \n",
    "    PathMatrix['Number of Circadian ECs']=PathMatrixECs['Number of Circadian ECs']\n",
    "    PathMatrix['Circadian ECs']=PathMatrixECs['Circadian ECs']\n",
    "    \n",
    "    #Drop rows which don't match any ECs (optional)\n",
    "    if Call=='Filter':\n",
    "        PathMatrixDrop=PathMatrix[PathMatrix['Number of Circadian ECs']!=0]\n",
    "        PathMatrixDrop=PathMatrixDrop[PathMatrixDrop['Gibbs Sum']<0]\n",
    "    if Call=='No Filter':\n",
    "        PathMatrixDrop=PathMatrix\n",
    "    \n",
    "    PathMatrixDrop=PathMatrixDrop.reset_index(drop=True)\n",
    "    \n",
    "    ##Sort by number of matches\n",
    "    PathMatrixDrop=PathMatrixDrop.sort_values('Gibbs Sum',ascending=True)\n",
    "    PathMatrixDrop=PathMatrixDrop.reset_index(drop=True)\n",
    "    \n",
    "    #Drop certain unwanted items\n",
    "    PathMatrixDrop=DropSpecifics(PathMatrixDrop)\n",
    "    \n",
    "    PathMatrixDrop=PathMatrixDrop.reset_index(drop=True)\n",
    "    \n",
    "    return(PathMatrixDrop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Drop certain metabolites that aren't relevant or don't match beyond a PubChem ID\n",
    "#More features can be added here depending on the organism, experimenters' needs etc..\n",
    "#This is very slow, may need a new way to do this if many more metabolites need to be added\n",
    "\n",
    "def DropSpecifics(PathMatrix):\n",
    "    \n",
    "    #Regex for certain ECs\n",
    "    EC1=re.compile('\\[\\'EC-4.1.2.22\\'\\]')\n",
    "    EC2=re.compile('\\[\\'EC-4.1.2.9\\'\\]')\n",
    "    EC3=re.compile('\\'EC-4.1.1.39\\'')\n",
    "    \n",
    "    DropIndex=[]\n",
    "    for rxn in range(len(PathMatrix)):\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and 'Sorbose' in x]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and 'AC1L1ARE' in x]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and 'AC1L19OT' in x]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and '498-23-7' in x]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and 'AAAHB' in x]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and 'ETHYLENE' in x]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and EC1.search(x)]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and EC2.search(x)]):\n",
    "            DropIndex.append(rxn)\n",
    "        if any([x for x in PathMatrix.iloc[rxn].values if type(x) is str and EC3.search(x)]):\n",
    "            DropIndex.append(rxn)\n",
    "    DropIndex=list(set(DropIndex))\n",
    "    PathMatrix=PathMatrix.drop(DropIndex)\n",
    "    PathMatrix=PathMatrix.reset_index(drop=True)\n",
    "    \n",
    "    return(PathMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleanup and remove duplicate rows\n",
    "def CleanAndRewrite(PathDF, rxnlength):\n",
    "    PathDF=PathDF.sort_values(PathDF.columns[-3],ascending=True)\n",
    "    PathDF=PathDF.drop_duplicates()\n",
    "    PathDF=PathDF.reset_index(drop=True)\n",
    "    #Goal - combine rows\n",
    "    colnames = []\n",
    "    \n",
    "    #rename columns\n",
    "    for rxn in reversed(range(1, rxnlength+2)):\n",
    "        colnames.append(['Reaction ' + str(rxn), 'Metabolite ' + str(rxn)])\n",
    "    colnames = list(chain.from_iterable(colnames)) + ['Gibbs'] + ['Circadian Enzyme Number'] + ['Circadian Enzymes']\n",
    "    colnames = colnames[1:]\n",
    "    PathDF.columns = colnames\n",
    "    \n",
    "    #Bring it all together: replace that row of the SerDF with the combined ECs\n",
    "    #Without the reaction columns, check if rows are duplicated\n",
    "    SubNames = [col for col in PathDF.columns if 'Metabolite' in col or 'Gibbs' in col]\n",
    "    SubSer = PathDF[SubNames]\n",
    "    SubSer = SubSer.drop_duplicates()\n",
    "    #if they are, then you need to combine the ECs in those rows\n",
    "    dupindex = [x for x in list(PathDF.index) if x not in list(SubSer.index)]\n",
    "    for dup in dupindex:\n",
    "        #find max value of an index from SerDF which is less than dupindex\n",
    "        serindex = (max([x for x in list(PathDF.index) if x < dup]))\n",
    "        SerMini = PathDF.loc[serindex:dup]\n",
    "        fillvals = []\n",
    "        \n",
    "        for col in SerMini.columns:\n",
    "            rxnlist = [x for x in SerMini[col]]\n",
    "            rxnlist = [re.sub('\\[|\\]|\\\"', '', str(x)) for x in rxnlist]\n",
    "            rxnlist = list(set(rxnlist))\n",
    "            fillvals.append(rxnlist)\n",
    "            \n",
    "        temp = pd.DataFrame([list((set(x))) for x in fillvals]).T\n",
    "        for val in range(len(temp.iloc[1])):\n",
    "            if temp.iloc[1][val] is not None:\n",
    "                temp.iloc[0][val] = temp.iloc[0][val] + \", \" + temp.iloc[1][val]\n",
    "        \n",
    "        temp.columns = PathDF.columns\n",
    "        SubNames = [col for col in temp.columns if 'Reaction' in col or 'Circadian Enzymes' in col]\n",
    "        for col in SubNames:\n",
    "            temp.iloc[0][col] = [x for x in list(temp[col]) if x is not None]\n",
    "        PathDF.iloc[serindex] = temp.iloc[0]\n",
    "    \n",
    "    PathDF = PathDF.drop(dupindex)\n",
    "    PathDF = PathDF.reset_index(drop = True)\n",
    "    \n",
    "    for number in range(len(PathDF)):\n",
    "        if type(PathDF.iloc[number]['Circadian Enzyme Number'])==str:\n",
    "            PathDF.iloc[number]['Circadian Enzyme Number'] = max(PathDF.iloc[number]['Circadian Enzyme Number'])\n",
    "    \n",
    "    #Clean hideous lists\n",
    "    for enz in range(len(PathDF)):\n",
    "        PathDF.iloc[enz]['Circadian Enzymes'] = list(set(re.split('\\,', re.sub('\\\\\\|\\'|\\[|\\]|\\\"| ', '', str(PathDF['Circadian Enzymes'][enz])))))\n",
    "    \n",
    "    return(PathDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the results of the isotopologue file of interest\n",
    "rxnlength=int(re.findall('(\\d+)Rxns', 'Serine M+3_Paths_9Rxns.csv')[0])\n",
    "MetabDF=pd.read_csv('Serine M+3_Paths_9Rxns.csv',header=None,error_bad_lines=False, names=list(range(rxnlength*2+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean up serine\n",
    "ParaNum = 2\n",
    "PathMatrixSplit=list(np.array_split(MetabDF,ParaNum)) #this could give an error if the number of rows is <16\n",
    "    \n",
    "pooler=mp.Pool(ParaNum)\n",
    "\n",
    "Call = list(np.repeat('Filter', ParaNum))\n",
    "#In case it exists already\n",
    "if 'Trimmed_Paths.csv' in os.listdir():\n",
    "    os.remove('Trimmed_Paths.csv')\n",
    "\n",
    "with open('Trimmed_Paths.csv','a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "    #written out to file as its being built\n",
    "        #for result in pooler.imap(MatchCircECHitsAndGetGibbsFilter,PathMatrixSplit):\n",
    "        for result in pooler.starmap(MatchCircECHitsAndGetGibbs,zip(PathMatrixSplit, Call)):\n",
    "            #Each result is a Pandas Object, so write it to csv\n",
    "            result.to_csv(fp,index=False,header=False)\n",
    "\n",
    "pooler.close()\n",
    "pooler.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Read back in to sort\n",
    "##Sort by number of matches\n",
    "PathDF=pd.read_csv('Trimmed_Paths.csv',header=None)\n",
    "Out=CleanAndRewrite(PathDF, rxnlength)\n",
    "Out=Out.T\n",
    "Out.columns=['Path ' + str(x) for x in list(Out.columns+1)]\n",
    "Out.to_csv('SerM3_Paths_9RxnsFilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#No filter, what was lost?\n",
    "rxnlength=int(re.findall('(\\d+)Rxns', 'Serine M+3_Paths_9Rxns.csv')[0])\n",
    "MetabDF=pd.read_csv('Serine M+3_Paths_9Rxns.csv',header=None,error_bad_lines=False, names=list(range(rxnlength*2+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean up serine\n",
    "ParaNum = 2\n",
    "PathMatrixSplit=list(np.array_split(MetabDF,ParaNum)) #this could give an error if the number of rows is <16\n",
    "    \n",
    "pooler=mp.Pool(ParaNum)\n",
    "\n",
    "Call = list(np.repeat('No Filter', ParaNum))\n",
    "#In case it exists already\n",
    "if 'Trimmed_Paths.csv' in os.listdir():\n",
    "    os.remove('Trimmed_Paths.csv')\n",
    "\n",
    "with open('Trimmed_Paths.csv','a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "    #written out to file as its being built\n",
    "        #for result in pooler.imap(MatchCircECHitsAndGetGibbsFilter,PathMatrixSplit):\n",
    "        for result in pooler.starmap(MatchCircECHitsAndGetGibbs,zip(PathMatrixSplit, Call)):\n",
    "            #Each result is a Pandas Object, so write it to csv\n",
    "            result.to_csv(fp,index=False,header=False)\n",
    "\n",
    "pooler.close()\n",
    "pooler.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Read back in to sort\n",
    "##Sort by number of matches\n",
    "PathDF=pd.read_csv('Trimmed_Paths.csv',header=None)\n",
    "Out=CleanAndRewrite(PathDF, rxnlength)\n",
    "Out=Out.T\n",
    "Out.columns=['Path ' + str(x) for x in list(Out.columns+1)]\n",
    "Out.to_csv('SerM3_Paths_9RxnsNoFilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the results of the isotopologue file of interest\n",
    "#MetabDF=pd.read_csv('Glutamine M+2_Paths_14Rxns.csv',header=None,error_bad_lines=False)\n",
    "rxnlength=int(re.findall('(\\d+)Rxns', 'Glutamine M+2_Paths_14Rxns.csv')[0])\n",
    "MetabDF=pd.read_csv('Glutamine M+2_Paths_14Rxns.csv', header=None, error_bad_lines=False, names=list(range(rxnlength*2+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Clean up gln\n",
    "ParaNum = 4\n",
    "PathMatrixSplit=list(np.array_split(MetabDF,ParaNum)) #this could give an error if the number of rows is <16\n",
    "    \n",
    "pooler=mp.Pool(ParaNum)\n",
    "\n",
    "Call = list(np.repeat('Filter', ParaNum))\n",
    "#In case it exists already\n",
    "if 'Trimmed_Paths.csv' in os.listdir():\n",
    "    os.remove('Trimmed_Paths.csv')\n",
    "\n",
    "with open('Trimmed_Paths.csv','a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "    #written out to file as its being built\n",
    "        #for result in pooler.imap(MatchCircECHitsAndGetGibbsFilter,PathMatrixSplit):\n",
    "        for result in pooler.starmap(MatchCircECHitsAndGetGibbs,zip(PathMatrixSplit, Call)):\n",
    "            #Each result is a Pandas Object, so write it to csv\n",
    "            result.to_csv(fp,index=False,header=False)\n",
    "\n",
    "pooler.close()\n",
    "pooler.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read back in to sort\n",
    "##Sort by number of matches\n",
    "PathDF=pd.read_csv('Trimmed_Paths.csv',header=None)\n",
    "Out=CleanAndRewrite(PathDF, rxnlength)\n",
    "Out=Out.T\n",
    "Out.columns=['Path ' + str(x) for x in list(Out.columns+1)]\n",
    "Out.to_csv('GlnM2_Paths_14RxnsFilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in the results of the isotopologue file of interest\n",
    "#MetabDF=pd.read_csv('Glutamine M+2_Paths_14Rxns.csv',header=None,error_bad_lines=False)\n",
    "rxnlength=int(re.findall('(\\d+)Rxns', 'Glutamine M+2_Paths_14Rxns.csv')[0])\n",
    "MetabDF=pd.read_csv('Glutamine M+2_Paths_14Rxns.csv', header=None, error_bad_lines=False, names=list(range(rxnlength*2+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean up gln\n",
    "ParaNum = 4\n",
    "PathMatrixSplit=list(np.array_split(MetabDF,ParaNum)) #this could give an error if the number of rows is <16\n",
    "    \n",
    "pooler=mp.Pool(ParaNum)\n",
    "\n",
    "Call = list(np.repeat('No Filter', ParaNum))\n",
    "#In case it exists already\n",
    "os.remove('Trimmed_Paths.csv')\n",
    "\n",
    "with open('Trimmed_Paths.csv','a') as fp: #originally 'w' - but append for looping through shorter path lengths\n",
    "    #written out to file as its being built\n",
    "        #for result in pooler.imap(MatchCircECHitsAndGetGibbsFilter,PathMatrixSplit):\n",
    "        for result in pooler.starmap(MatchCircECHitsAndGetGibbs,zip(PathMatrixSplit, Call)):\n",
    "            #Each result is a Pandas Object, so write it to csv\n",
    "            result.to_csv(fp,index=False,header=False)\n",
    "\n",
    "pooler.close()\n",
    "pooler.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Read back in to sort\n",
    "##Sort by number of matches\n",
    "PathDF=pd.read_csv('Trimmed_Paths.csv',header=None)\n",
    "Out=CleanAndRewrite(PathDF, rxnlength)\n",
    "Out=Out.T\n",
    "Out.columns=['Path ' + str(x) for x in list(Out.columns+1)]\n",
    "Out.to_csv('GlnM2_Paths_14RxnsNoFilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
